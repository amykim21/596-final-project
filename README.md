# 596-final-project
LLaMA (Large Language Model Meta AI) efficiency project proposal
Amy Kim, Charley Kim

Abstract
With the public, free release of ChatGPT 3.5 and other large language models, AI is being trained on larger and larger data sets with billions and potentially trillions of data points. Our project proposes running queries on different sizes of Metaâ€™s Large Language Model Meta AI (LLaMA) using a different number of cores to measure and compare the efficiency of the models. As AI and language models are applied to more aspects of technological and daily life, developers turn to user larger data and training sets, making it important to balance the accuracy of language models with their size and efficiency. 

