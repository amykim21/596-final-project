# 596-final-project
LLaMA (Large Language Model Meta AI) efficiency project proposal


Amy Kim, Charley Kim

Abstract


With the public, free release of ChatGPT 3.5 and other large language models, AI is being trained on larger and larger data sets with billions and potentially trillions of data points. Our project proposes running queries on different sizes of Metaâ€™s Large Language Model Meta AI (LLaMA) using a different number of cores to measure and compare the efficiency of the models. As AI and language models are applied to more aspects of technological and daily life, developers turn to user larger data and training sets, making it important to balance the accuracy of language models with their size and efficiency. 

![download](https://github.com/amykim21/596-final-project/assets/69876199/06f7d8fb-a11a-4d34-82cd-9f6dcc52d0c3)



<img width="615" alt="Screen Shot 2023-12-01 at 3 33 53 PM" src="https://github.com/amykim21/596-final-project/assets/46797363/e13bd014-a81c-4a47-9fff-525d8928c06c">



This project proposal aims to measure if the efficiency of large language models can be improved further, as trends have demonstrated data sets growing tens of times larger per iteration. 
ChatGPT and other large language models (LLM) are not open source, making it difficult to modify code and run the models on different cores. LLaMA is open source and available to the public, allowing developers to apply for a license and modify code. 
We plan to use a performance/load testing software tool which is used in industry to measure the efficiency of LLaMA. Some options that we are considering are Apache JMeter, LoadRunner, or other software tools specialized for large language models.
